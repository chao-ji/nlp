{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this ipython notebook, we evaluate bigram language model using a subset from brown corpus in nltk.\n",
    "\n",
    "We first train the bigram language model with the orignal bigram sequences.\n",
    "\n",
    "The trained bigram model was evaluated on \n",
    "\n",
    "1. Test corpus where invidual tokens were **shuffled**, i.e. [\"I\", \"have\", \"an\", \"apple\"] ==> [\"an\", \"have\", \"apple\", \"I\"], which results in bigram sequences [(\"an\", \"have\"), (\"have\", \"apple\"), (\"apple\", \"I\")]\n",
    "    \n",
    "2. Test corpus with the order of tokens **unchanged**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "from nltk.corpus import brown\n",
    "from language_model import *\n",
    "import copy\n",
    "\n",
    "# read sentences in \"fiction\" in brown corpus\n",
    "corpus = list(brown.sents(categories=\"fiction\"))\n",
    "len_corpus = len(corpus)\n",
    "\n",
    "bigram_fwd = []\n",
    "bigram_shf = []\n",
    "unigram_fwd = []\n",
    "unigram_shf = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Thirty-three'],\n",
       " [u'Scotty', u'did', u'not', u'go', u'back', u'to', u'school', u'.']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "For each of the ten iterations, shuffle the corpus.\n",
    "Take the first 80% as training corpus and remaining 20% as test.\n",
    "\"\"\" \n",
    "for k in range(10):\n",
    "    np.random.shuffle(corpus)\n",
    "    print k\n",
    "\n",
    "    corpus_train = corpus[:int(len_corpus * 0.8)]\n",
    "    corpus_test = corpus[int(len_corpus * 0.8):]\n",
    "    \n",
    "    # find the list of unique tokens in training corpus, which is used \n",
    "    # to find the number of out-of-vocabulary words later\n",
    "    corpus_train_tokens = set(itertools.chain(*corpus_train))\n",
    "    corpus_test_tokens = list(itertools.chain(*corpus_test))\n",
    "\n",
    "    # forward sequences (order unchanged)\n",
    "    \n",
    "    # bigram\n",
    "    bigram = Bigram(special_token=True)\n",
    "    # oov: out-of-vocabulary words\n",
    "    oov = filter(lambda token: token not in corpus_train_tokens, corpus_test_tokens)\n",
    "    # `len(set(oov))`: number of oov\n",
    "    bigram.fit(corpus_train, len(set(oov)))\n",
    "    # for each sentence in `corpus_test` (a list of str), compute the log-probability score \n",
    "    logprob = map(lambda tokens: bigram.predict(tokens), corpus_test)\n",
    "    # save the mean of the `logprob` for the `k`th iteration\n",
    "    bigram_fwd.append(np.mean(logprob))\n",
    "\n",
    "    # repated the above for unigram model \n",
    "    unigram = Unigram(special_token=False)\n",
    "    unigram.fit(corpus_train, len(set(oov)))\n",
    "    logprob = map(lambda tokens: unigram.predict(tokens), corpus_test)\n",
    "    unigram_fwd.append(np.mean(logprob))\n",
    "\n",
    "    # shuffle tokens in sentences in text corpus\n",
    "    corpus_test_shf = copy.deepcopy(corpus_test)\n",
    "    for x in corpus_test_shf:\n",
    "        np.random.shuffle(x)\n",
    "\n",
    "    # repeat the analysis on shuffled tokens\n",
    "    # on bigram\n",
    "    bigram = Bigram(special_token=True)\n",
    "    oov = filter(lambda token: token not in corpus_train_tokens, corpus_test_tokens)\n",
    "    bigram.fit(corpus_train, len(set(oov)))\n",
    "    logprob = map(lambda tokens: bigram.predict(tokens), corpus_test_shf)\n",
    "    bigram_shf.append(np.mean(logprob))\n",
    "\n",
    "    # on unigram\n",
    "    unigram = Unigram(special_token=False)\n",
    "    unigram.fit(corpus_train, len(set(oov)))\n",
    "    logprob = map(lambda tokens: unigram.predict(tokens), corpus_test_shf)\n",
    "    unigram_shf.append(np.mean(logprob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average log-prob of bigrams (order of tokens unchaged): -5.34970822898\n"
     ]
    }
   ],
   "source": [
    "print \"Average log-prob of bigrams (order of tokens unchaged):\", np.mean(bigram_fwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average log-prob of bigrams (order of tokens shuffled): -6.92859091893\n"
     ]
    }
   ],
   "source": [
    "print \"Average log-prob of bigrams (order of tokens shuffled):\", np.mean(bigram_shf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average log-prob of unigrams (order of tokens unchaged): -6.56173921049\n"
     ]
    }
   ],
   "source": [
    "print \"Average log-prob of unigrams (order of tokens unchaged):\", np.mean(unigram_fwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average log-prob of unigrams (order of tokens shuffled): -6.56173921049\n"
     ]
    }
   ],
   "source": [
    "print \"Average log-prob of unigrams (order of tokens shuffled):\", np.mean(unigram_shf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the log-prob of bigrams where the order of tokens were shuffled is much lower than the case where the order of tokens were unchanged, which makes sense because the bigram models takes into account the relative order of adjacent tokens in a sentence.\n",
    "\n",
    "In contrast, the comparison between shuffled and unchanged sequence of tokens resulted in exactly the same log-probablity, because unigram model only counts the number of unique tokens."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
